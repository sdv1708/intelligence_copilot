{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Executive Intelligence Copilot - Colab Pro Deployment\n",
        "\n",
        "**Run your Intelligence Copilot on Google Colab with GPU acceleration!**\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Setup Instructions\n",
        "\n",
        "1. **Enable GPU**: Go to `Runtime` ‚Üí `Change runtime type` ‚Üí Select `T4 GPU`\n",
        "2. **Run all cells** in order (Ctrl+F9 or Runtime ‚Üí Run all)\n",
        "3. **Enter your API keys** when prompted\n",
        "4. **Click the ngrok URL** to access your app\n",
        "\n",
        "---\n",
        "\n",
        "### Performance Expectations\n",
        "\n",
        "| Operation | CPU (Local) | GPU (Colab Pro) |\n",
        "|-----------|-------------|------------------|\n",
        "| Model Load | 5-10s | 2-3s |\n",
        "| Embed 10 pages | 15-30s | 1-2s |\n",
        "| **Total speedup** | 1x | **15-20x faster** |\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"‚úÖ GPU detected!\")\n",
        "    print(f\"   Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Go to Runtime ‚Üí Change runtime type ‚Üí Select GPU\")\n",
        "    print(\"   Note: GPU will give 15-20x faster performance!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone Repository or Upload Project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Option 1: Upload your entire project folder using the folder icon on the left\n",
        "# Option 2: Clone from GitHub (uncomment and edit)\n",
        "# !git clone https://github.com/YOUR_USERNAME/intelligence_copilot.git\n",
        "\n",
        "# Change to project directory (adjust path if needed)\n",
        "PROJECT_PATH = \"/content/intelligence_copilot\"\n",
        "\n",
        "if os.path.exists(PROJECT_PATH):\n",
        "    os.chdir(PROJECT_PATH)\n",
        "    print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"‚ùå Project not found at {PROJECT_PATH}\")\n",
        "    print(\"   Please upload your project folder or clone from GitHub\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all requirements\n",
        "%pip install -q -r requirements.txt\n",
        "\n",
        "# Install ngrok for public URL\n",
        "%pip install -q pyngrok\n",
        "\n",
        "print(\"‚úÖ Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Configure API Keys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "print(\"üîë Enter your API keys\")\n",
        "print(\"   (Leave blank to skip if not using that provider)\\n\")\n",
        "\n",
        "# LLM Provider Selection\n",
        "print(\"Select your LLM provider:\")\n",
        "print(\"  1. Gemini (Recommended - Free tier available)\")\n",
        "print(\"  2. OpenAI\")\n",
        "print(\"  3. Anthropic\\n\")\n",
        "\n",
        "provider_choice = input(\"Enter choice (1/2/3): \") or \"1\"\n",
        "provider_map = {\"1\": \"gemini\", \"2\": \"openai\", \"3\": \"anthropic\"}\n",
        "llm_provider = provider_map.get(provider_choice, \"gemini\")\n",
        "\n",
        "os.environ[\"LLM_PROVIDER\"] = llm_provider\n",
        "\n",
        "# Get API keys based on provider\n",
        "if llm_provider == \"gemini\":\n",
        "    gemini_key = getpass(\"Enter Gemini API key: \")\n",
        "    os.environ[\"GEMINI_API_KEY\"] = gemini_key\n",
        "    print(\"   Get free key: https://makersuite.google.com/app/apikey\")\n",
        "elif llm_provider == \"openai\":\n",
        "    openai_key = getpass(\"Enter OpenAI API key: \")\n",
        "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "elif llm_provider == \"anthropic\":\n",
        "    anthropic_key = getpass(\"Enter Anthropic API key: \")\n",
        "    os.environ[\"ANTHROPIC_API_KEY\"] = anthropic_key\n",
        "\n",
        "# Ngrok token for public URL\n",
        "print(\"\\nüì° Ngrok Setup (for public URL)\")\n",
        "print(\"   Sign up free: https://dashboard.ngrok.com/signup\")\n",
        "ngrok_token = getpass(\"Enter ngrok authtoken: \")\n",
        "os.environ[\"NGROK_TOKEN\"] = ngrok_token\n",
        "\n",
        "print(\"\\n‚úÖ Environment configured!\")\n",
        "print(f\"   LLM Provider: {llm_provider.upper()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Verify GPU Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick test of GPU acceleration\n",
        "from core.embed import get_device, get_model\n",
        "\n",
        "device = get_device()\n",
        "print(f\"üöÄ Embedding device: {device.upper()}\")\n",
        "\n",
        "if device == \"cuda\":\n",
        "    print(\"‚úÖ GPU acceleration enabled!\")\n",
        "    print(\"   Expect 15-20x faster embeddings\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Running on CPU\")\n",
        "    print(\"   Consider enabling GPU for better performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Launch Streamlit App üöÄ\n",
        "\n",
        "**Run this cell and click the ngrok URL to access your app!**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(os.environ[\"NGROK_TOKEN\"])\n",
        "\n",
        "# Start Streamlit in background\n",
        "def run_streamlit():\n",
        "    subprocess.run([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\", \"--server.headless=true\"])\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "# Wait for Streamlit to start\n",
        "print(\"üöÄ Starting Streamlit...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Create ngrok tunnel\n",
        "public_url = ngrok.connect(8501, bind_tls=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ APP IS RUNNING!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüåê Access your app at: {public_url}\")\n",
        "print(\"\\nüìù Tips:\")\n",
        "print(\"   - Upload documents to test GPU-accelerated embeddings\")\n",
        "print(\"   - Monitor GPU usage in the Colab Runtime tab\")\n",
        "print(\"   - Keep this notebook running to maintain the session\")\n",
        "print(\"   - Session lasts as long as this cell is running\")\n",
        "print(\"\\nüõë To stop: Click the stop button or Kernel ‚Üí Interrupt\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Keep running\n",
        "try:\n",
        "    thread.join()\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüõë Shutting down...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Monitor GPU Usage (Optional)\n",
        "\n",
        "Run this while using the app to verify GPU acceleration is working!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell while using the app to monitor GPU\n",
        "!nvidia-smi\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
